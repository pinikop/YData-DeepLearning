{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using ConvNet\n",
    "Do the same, using a ConvNet.  \n",
    "The ConvNet should get as input a 2D matrix where each column is an embedding vector of a single word, and words are in order. Use zero padding so that all matrices have a similar length.  \n",
    "Some songs might be very long. Trim them so you keep a maximum of 128 words (after cleaning stop words and rare words).  \n",
    "Initialize the embedding layer using the word vectors that you've trained before, but allow them to change during training.  \n",
    "\n",
    "Extra: Try training the ConvNet with 2 slight modifications:\n",
    "1. freezing the the weights trained using Word2vec (preventing it from updating)\n",
    "1. random initialization of the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are encouraged to try this question on your own.  \n",
    "\n",
    "You might prefer to get ideas from the paper \"Convolutional Neural Networks for Sentence Classification\" (Kim 2014, [link](https://arxiv.org/abs/1408.5882)).\n",
    "\n",
    "There are several implementations of the paper code in PyTorch online (see for example [this repo](https://github.com/prakashpandey9/Text-Classification-Pytorch) for a PyTorch implementation of CNN and other architectures for text classification). If you get stuck, they might provide you with a reference for your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'lyrics.csv'\n",
    "DATA_DIR = 'data'\n",
    "MODELS_DIR = 'models'\n",
    "\n",
    "MAX_N_WORDS = 128\n",
    "MAX_FEATURES = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    \"\"\"Callback to log information about training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "w2v = Word2Vec.load(os.path.join(MODELS_DIR, 'w2v.model'))\n",
    "# w2v.init_sims(replace=True)\n",
    "\n",
    "df = pd.read_pickle(os.path.join(DATA_DIR, 'lyrics_df.pkl'))\n",
    "df.drop(df[df.genre == 'Not Available'].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will prepare the data for training - we split the data into train and test, crop lyrics to the the required length and pad them if needed, and convert each lyrics to a one hot, 2d representation using CountVectorizor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_lyrics(row, n_words):\n",
    "    return row.clean_lyrics[:n_words]\n",
    "\n",
    "\n",
    "def lyrics_to_embedding(row, n_words, n_features, count_vect):\n",
    "    \n",
    "    embedding = count_vect.transform(row).toarray().argmax(axis=1)\n",
    "    \n",
    "    n_vects = embedding.shape[0]\n",
    "    if n_vects < n_words:\n",
    "        embedding = np.append(embedding, \n",
    "                              np.zeros(n_words - n_vects),\n",
    "                              axis=0)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = df.iloc[idx].copy()\n",
    "df['genre_code'] = df.genre.astype('category').cat.codes\n",
    "df['cropped_lyrics'] = df.apply(clip_lyrics, args=(MAX_N_WORDS,), axis=1)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.cropped_lyrics, \n",
    "                                                    df.genre_code, \n",
    "                                                    test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 8.63 s, sys: 64.6 ms, total: 8.69 s\n",
      "Wall time: 8.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = w2v.wv.index2entity[:MAX_FEATURES]\n",
    "count_vect = CountVectorizer(vocabulary=vocab).fit(X_train.str.join(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 26s, sys: 64.8 ms, total: 1min 26s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = X_train.apply(lyrics_to_embedding, args=(MAX_N_WORDS, MAX_FEATURES, count_vect))\n",
    "X_test = X_test.apply(lyrics_to_embedding, args=(MAX_N_WORDS, MAX_FEATURES, count_vect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we convert the processed data into dataloaders to be used for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_target = torch.tensor(y_train.values).long()\n",
    "train = torch.tensor(np.stack(X_train.values)).long()\n",
    "train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "train_loader = data_utils.DataLoader(dataset=train_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_target = torch.tensor(y_test.values).long()\n",
    "test = torch.tensor(np.stack(X_test.values)).long()\n",
    "test_tensor = data_utils.TensorDataset(test, test_target) \n",
    "test_loader = data_utils.DataLoader(dataset=test_tensor, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our model - the model will receive as input a 2d, one hot representation of the lyrics, as a sequence, and will make use of the word2vec embeddings, allowing them to change during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, n_labels, initial_weights=None, freeze_weights=False,\n",
    "                 in_channels=1, out_channels=100, \n",
    "                 kernels=[3,4,5], padding=0, stride=1, \n",
    "                 keep_probab = 0.5,\n",
    "                ):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.allow_grad = True\n",
    "        \n",
    "        if initial_weights is None:\n",
    "            raise NotImplementedError('Random Initialization Not Implemented Yet')\n",
    "        else:\n",
    "            self.init_weights = torch.tensor(initial_weights, dtype=torch.float)\n",
    "            self.allow_grad = not freeze_weights\n",
    "            vocab_size, embedding_length = initial_weights.shape\n",
    "\n",
    "        \n",
    "        self.kernels = np.asarray(kernels)\n",
    "        \n",
    "#         self.lr = lr        \n",
    "        \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weight = nn.Parameter(self.init_weights, requires_grad=self.allow_grad)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, \n",
    "                               kernel_size=(kernels[0], embedding_length), \n",
    "                               stride=embedding_length, \n",
    "                               )\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, \n",
    "                               kernel_size=(kernels[1], embedding_length), \n",
    "                               stride=embedding_length, \n",
    "                               )\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels, out_channels, \n",
    "                               kernel_size=(kernels[2], embedding_length), \n",
    "                               stride=embedding_length, \n",
    "                               )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=keep_probab)\n",
    "        self.label = nn.Linear(len(kernels)*out_channels, n_labels)\n",
    "        \n",
    "    def _conv_block(self, x, conv_layer):\n",
    "        conv_out = conv_layer(x)  # conv_out.size() = (batch_size, out_channels, dim, 1)\n",
    "        activation = F.relu(conv_out.squeeze(3))  # activation.size() = (batch_size, out_channels, dim1)\n",
    "        max_out = F.max_pool1d(activation, activation.size()[2]).squeeze(2)  # maxpool_out.size() = (batch_size, out_channels)\n",
    "        return max_out\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        x = self.word_embeddings(x)\n",
    "        out = x.unsqueeze(1)\n",
    "        max_out1 = self._conv_block(out, self.conv1)\n",
    "        max_out2 = self._conv_block(out, self.conv2)\n",
    "        max_out3 = self._conv_block(out, self.conv3)\n",
    "\n",
    "        all_out = torch.cat((max_out1, max_out2, max_out3), 1)\n",
    "        all_out = all_out.view(all_out.size(0), -1)\n",
    "        fc_in = self.dropout(all_out)\n",
    "        \n",
    "        scores = self.label(fc_in)\n",
    "        \n",
    "        scores = F.softmax(scores, dim=1)\n",
    "        return scores\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(123)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = np.unique(y_train).size\n",
    "bow_vocab = count_vect.vocabulary  \n",
    "\n",
    "model = ConvNet(initial_weights=w2v.wv[bow_vocab], n_labels=output_size).to(device)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()  # set evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.cross_entropy(output, target, size_average=False).item() # sum up batch loss\n",
    "            pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "            correct += (torch.max(pred, 1)[1].view(target.size()).data == target.data).sum().item()\n",
    "            #correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epoch, log_interval=200):\n",
    "    model.train()  # set training mode\n",
    "    \n",
    "    iteration = 0\n",
    "    for ep in range(epoch):\n",
    "        start = time.time()\n",
    "\n",
    "        for batch_idx, (X, target) in enumerate(train_loader):\n",
    "            # bring data to the computing device, e.g. GPU\n",
    "            X, target = Variable(X).to(device), Variable(target).to(device)\n",
    "            # forward pass\n",
    "            output = model(X)\n",
    "            # compute loss: negative log-likelihood\n",
    "            loss = F.cross_entropy(output, target)\n",
    "            \n",
    "            # backward pass\n",
    "            # clear the gradients of all tensors being optimized.\n",
    "            optimizer.zero_grad()\n",
    "            # accumulate (i.e. add) the gradients from this forward pass\n",
    "            loss.backward()\n",
    "            # performs a single optimization step (parameter update)\n",
    "            optimizer.step()\n",
    "            \n",
    "            if iteration % log_interval == 0:\n",
    "                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                    ep, batch_idx * len(X), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader), loss.item()))\n",
    "            iteration += 1\n",
    "            \n",
    "        end = time.time()\n",
    "        print('{:.2f}s'.format(end-start))\n",
    "        test() # evaluate at the end of epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/190551 (0%)]\tLoss: 2.398219\n",
      "Train Epoch: 0 [12800/190551 (7%)]\tLoss: 2.125164\n",
      "Train Epoch: 0 [25600/190551 (13%)]\tLoss: 2.043904\n",
      "Train Epoch: 0 [38400/190551 (20%)]\tLoss: 2.121514\n",
      "Train Epoch: 0 [51200/190551 (27%)]\tLoss: 2.011915\n",
      "Train Epoch: 0 [64000/190551 (34%)]\tLoss: 2.027268\n",
      "Train Epoch: 0 [76800/190551 (40%)]\tLoss: 2.152488\n",
      "Train Epoch: 0 [89600/190551 (47%)]\tLoss: 2.136848\n",
      "Train Epoch: 0 [102400/190551 (54%)]\tLoss: 2.121217\n",
      "Train Epoch: 0 [115200/190551 (60%)]\tLoss: 2.105567\n",
      "Train Epoch: 0 [128000/190551 (67%)]\tLoss: 2.043062\n",
      "Train Epoch: 0 [140800/190551 (74%)]\tLoss: 2.168046\n",
      "Train Epoch: 0 [153600/190551 (81%)]\tLoss: 2.136788\n",
      "Train Epoch: 0 [166400/190551 (87%)]\tLoss: 1.980548\n",
      "Train Epoch: 0 [179200/190551 (94%)]\tLoss: 2.074297\n",
      "12.68s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pini/opt/anaconda3/envs/py4dp/lib/python3.6/site-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 2.0920, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 1 [1408/190551 (1%)]\tLoss: 2.168052\n",
      "Train Epoch: 1 [14208/190551 (7%)]\tLoss: 2.058660\n",
      "Train Epoch: 1 [27008/190551 (14%)]\tLoss: 2.058419\n",
      "Train Epoch: 1 [39808/190551 (21%)]\tLoss: 2.056599\n",
      "Train Epoch: 1 [52608/190551 (28%)]\tLoss: 1.954655\n",
      "Train Epoch: 1 [65408/190551 (34%)]\tLoss: 2.101571\n",
      "Train Epoch: 1 [78208/190551 (41%)]\tLoss: 2.008793\n",
      "Train Epoch: 1 [91008/190551 (48%)]\tLoss: 2.057933\n",
      "Train Epoch: 1 [103808/190551 (54%)]\tLoss: 2.101366\n",
      "Train Epoch: 1 [116608/190551 (61%)]\tLoss: 2.076693\n",
      "Train Epoch: 1 [129408/190551 (68%)]\tLoss: 2.047946\n",
      "Train Epoch: 1 [142208/190551 (75%)]\tLoss: 2.052704\n",
      "Train Epoch: 1 [155008/190551 (81%)]\tLoss: 2.067844\n",
      "Train Epoch: 1 [167808/190551 (88%)]\tLoss: 2.087446\n",
      "Train Epoch: 1 [180608/190551 (95%)]\tLoss: 2.192807\n",
      "11.92s\n",
      "\n",
      "Test set: Average loss: 2.0650, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 2 [2816/190551 (1%)]\tLoss: 2.111047\n",
      "Train Epoch: 2 [15616/190551 (8%)]\tLoss: 2.037441\n",
      "Train Epoch: 2 [28416/190551 (15%)]\tLoss: 2.140664\n",
      "Train Epoch: 2 [41216/190551 (22%)]\tLoss: 2.084849\n",
      "Train Epoch: 2 [54016/190551 (28%)]\tLoss: 2.085248\n",
      "Train Epoch: 2 [66816/190551 (35%)]\tLoss: 2.011080\n",
      "Train Epoch: 2 [79616/190551 (42%)]\tLoss: 2.043101\n",
      "Train Epoch: 2 [92416/190551 (48%)]\tLoss: 2.126726\n",
      "Train Epoch: 2 [105216/190551 (55%)]\tLoss: 2.119227\n",
      "Train Epoch: 2 [118016/190551 (62%)]\tLoss: 2.158495\n",
      "Train Epoch: 2 [130816/190551 (69%)]\tLoss: 2.037364\n",
      "Train Epoch: 2 [143616/190551 (75%)]\tLoss: 2.089245\n",
      "Train Epoch: 2 [156416/190551 (82%)]\tLoss: 2.154775\n",
      "Train Epoch: 2 [169216/190551 (89%)]\tLoss: 2.105780\n",
      "Train Epoch: 2 [182016/190551 (96%)]\tLoss: 2.070779\n",
      "12.43s\n",
      "\n",
      "Test set: Average loss: 2.0643, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 3 [4224/190551 (2%)]\tLoss: 1.933054\n",
      "Train Epoch: 3 [17024/190551 (9%)]\tLoss: 2.128161\n",
      "Train Epoch: 3 [29824/190551 (16%)]\tLoss: 2.027051\n",
      "Train Epoch: 3 [42624/190551 (22%)]\tLoss: 2.017008\n",
      "Train Epoch: 3 [55424/190551 (29%)]\tLoss: 2.030060\n",
      "Train Epoch: 3 [68224/190551 (36%)]\tLoss: 1.984482\n",
      "Train Epoch: 3 [81024/190551 (43%)]\tLoss: 2.093033\n",
      "Train Epoch: 3 [93824/190551 (49%)]\tLoss: 2.073688\n",
      "Train Epoch: 3 [106624/190551 (56%)]\tLoss: 2.129950\n",
      "Train Epoch: 3 [119424/190551 (63%)]\tLoss: 2.059937\n",
      "Train Epoch: 3 [132224/190551 (69%)]\tLoss: 2.110079\n",
      "Train Epoch: 3 [145024/190551 (76%)]\tLoss: 2.085598\n",
      "Train Epoch: 3 [157824/190551 (83%)]\tLoss: 2.064339\n",
      "Train Epoch: 3 [170624/190551 (90%)]\tLoss: 2.074141\n",
      "Train Epoch: 3 [183424/190551 (96%)]\tLoss: 2.063888\n",
      "12.20s\n",
      "\n",
      "Test set: Average loss: 2.0638, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 4 [5632/190551 (3%)]\tLoss: 2.008612\n",
      "Train Epoch: 4 [18432/190551 (10%)]\tLoss: 2.105604\n",
      "Train Epoch: 4 [31232/190551 (16%)]\tLoss: 2.019492\n",
      "Train Epoch: 4 [44032/190551 (23%)]\tLoss: 2.047707\n",
      "Train Epoch: 4 [56832/190551 (30%)]\tLoss: 2.056914\n",
      "Train Epoch: 4 [69632/190551 (37%)]\tLoss: 2.121518\n",
      "Train Epoch: 4 [82432/190551 (43%)]\tLoss: 2.036971\n",
      "Train Epoch: 4 [95232/190551 (50%)]\tLoss: 2.106875\n",
      "Train Epoch: 4 [108032/190551 (57%)]\tLoss: 2.083115\n",
      "Train Epoch: 4 [120832/190551 (63%)]\tLoss: 2.090857\n",
      "Train Epoch: 4 [133632/190551 (70%)]\tLoss: 2.060342\n",
      "Train Epoch: 4 [146432/190551 (77%)]\tLoss: 1.944796\n",
      "Train Epoch: 4 [159232/190551 (84%)]\tLoss: 2.012039\n",
      "Train Epoch: 4 [172032/190551 (90%)]\tLoss: 2.096163\n",
      "Train Epoch: 4 [184832/190551 (97%)]\tLoss: 1.918466\n",
      "11.91s\n",
      "\n",
      "Test set: Average loss: 2.0640, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 5 [7040/190551 (4%)]\tLoss: 2.166182\n",
      "Train Epoch: 5 [19840/190551 (10%)]\tLoss: 2.043687\n",
      "Train Epoch: 5 [32640/190551 (17%)]\tLoss: 2.044211\n",
      "Train Epoch: 5 [45440/190551 (24%)]\tLoss: 2.090627\n",
      "Train Epoch: 5 [58240/190551 (31%)]\tLoss: 2.136063\n",
      "Train Epoch: 5 [71040/190551 (37%)]\tLoss: 2.161929\n",
      "Train Epoch: 5 [83840/190551 (44%)]\tLoss: 2.104569\n",
      "Train Epoch: 5 [96640/190551 (51%)]\tLoss: 1.989453\n",
      "Train Epoch: 5 [109440/190551 (57%)]\tLoss: 2.142125\n",
      "Train Epoch: 5 [122240/190551 (64%)]\tLoss: 2.032053\n",
      "Train Epoch: 5 [135040/190551 (71%)]\tLoss: 1.986203\n",
      "Train Epoch: 5 [147840/190551 (78%)]\tLoss: 2.260421\n",
      "Train Epoch: 5 [160640/190551 (84%)]\tLoss: 2.042106\n",
      "Train Epoch: 5 [173440/190551 (91%)]\tLoss: 2.058461\n",
      "Train Epoch: 5 [186240/190551 (98%)]\tLoss: 2.027468\n",
      "12.20s\n",
      "\n",
      "Test set: Average loss: 2.0637, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 6 [8448/190551 (4%)]\tLoss: 1.995060\n",
      "Train Epoch: 6 [21248/190551 (11%)]\tLoss: 1.981112\n",
      "Train Epoch: 6 [34048/190551 (18%)]\tLoss: 1.950342\n",
      "Train Epoch: 6 [46848/190551 (25%)]\tLoss: 2.014062\n",
      "Train Epoch: 6 [59648/190551 (31%)]\tLoss: 2.094079\n",
      "Train Epoch: 6 [72448/190551 (38%)]\tLoss: 2.072951\n",
      "Train Epoch: 6 [85248/190551 (45%)]\tLoss: 2.035131\n",
      "Train Epoch: 6 [98048/190551 (51%)]\tLoss: 2.006848\n",
      "Train Epoch: 6 [110848/190551 (58%)]\tLoss: 2.044785\n",
      "Train Epoch: 6 [123648/190551 (65%)]\tLoss: 2.067059\n",
      "Train Epoch: 6 [136448/190551 (72%)]\tLoss: 2.094395\n",
      "Train Epoch: 6 [149248/190551 (78%)]\tLoss: 2.012451\n",
      "Train Epoch: 6 [162048/190551 (85%)]\tLoss: 2.153220\n",
      "Train Epoch: 6 [174848/190551 (92%)]\tLoss: 2.091412\n",
      "Train Epoch: 6 [187648/190551 (98%)]\tLoss: 2.027621\n",
      "12.12s\n",
      "\n",
      "Test set: Average loss: 2.0637, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 7 [9856/190551 (5%)]\tLoss: 2.043942\n",
      "Train Epoch: 7 [22656/190551 (12%)]\tLoss: 2.058991\n",
      "Train Epoch: 7 [35456/190551 (19%)]\tLoss: 2.058311\n",
      "Train Epoch: 7 [48256/190551 (25%)]\tLoss: 2.047196\n",
      "Train Epoch: 7 [61056/190551 (32%)]\tLoss: 1.980748\n",
      "Train Epoch: 7 [73856/190551 (39%)]\tLoss: 2.042267\n",
      "Train Epoch: 7 [86656/190551 (45%)]\tLoss: 2.011756\n",
      "Train Epoch: 7 [99456/190551 (52%)]\tLoss: 2.062402\n",
      "Train Epoch: 7 [112256/190551 (59%)]\tLoss: 2.089454\n",
      "Train Epoch: 7 [125056/190551 (66%)]\tLoss: 2.043135\n",
      "Train Epoch: 7 [137856/190551 (72%)]\tLoss: 2.102179\n",
      "Train Epoch: 7 [150656/190551 (79%)]\tLoss: 2.078079\n",
      "Train Epoch: 7 [163456/190551 (86%)]\tLoss: 1.902795\n",
      "Train Epoch: 7 [176256/190551 (92%)]\tLoss: 2.125159\n",
      "Train Epoch: 7 [189056/190551 (99%)]\tLoss: 2.135939\n",
      "12.03s\n",
      "\n",
      "Test set: Average loss: 2.0636, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 8 [11264/190551 (6%)]\tLoss: 1.980850\n",
      "Train Epoch: 8 [24064/190551 (13%)]\tLoss: 1.964975\n",
      "Train Epoch: 8 [36864/190551 (19%)]\tLoss: 2.062166\n",
      "Train Epoch: 8 [49664/190551 (26%)]\tLoss: 2.035363\n",
      "Train Epoch: 8 [62464/190551 (33%)]\tLoss: 2.120533\n",
      "Train Epoch: 8 [75264/190551 (39%)]\tLoss: 2.073474\n",
      "Train Epoch: 8 [88064/190551 (46%)]\tLoss: 2.137016\n",
      "Train Epoch: 8 [100864/190551 (53%)]\tLoss: 2.120395\n",
      "Train Epoch: 8 [113664/190551 (60%)]\tLoss: 1.968572\n",
      "Train Epoch: 8 [126464/190551 (66%)]\tLoss: 2.042255\n",
      "Train Epoch: 8 [139264/190551 (73%)]\tLoss: 2.027018\n",
      "Train Epoch: 8 [152064/190551 (80%)]\tLoss: 2.059879\n",
      "Train Epoch: 8 [164864/190551 (87%)]\tLoss: 2.045667\n",
      "Train Epoch: 8 [177664/190551 (93%)]\tLoss: 2.129776\n",
      "Train Epoch: 8 [190464/190551 (100%)]\tLoss: 2.035025\n",
      "12.10s\n",
      "\n",
      "Test set: Average loss: 2.0637, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 9 [12672/190551 (7%)]\tLoss: 2.105570\n",
      "Train Epoch: 9 [25472/190551 (13%)]\tLoss: 2.058672\n",
      "Train Epoch: 9 [38272/190551 (20%)]\tLoss: 2.060230\n",
      "Train Epoch: 9 [51072/190551 (27%)]\tLoss: 2.058531\n",
      "Train Epoch: 9 [63872/190551 (34%)]\tLoss: 1.996127\n",
      "Train Epoch: 9 [76672/190551 (40%)]\tLoss: 2.106263\n",
      "Train Epoch: 9 [89472/190551 (47%)]\tLoss: 1.980879\n",
      "Train Epoch: 9 [102272/190551 (54%)]\tLoss: 2.011884\n",
      "Train Epoch: 9 [115072/190551 (60%)]\tLoss: 2.034263\n",
      "Train Epoch: 9 [127872/190551 (67%)]\tLoss: 2.073033\n",
      "Train Epoch: 9 [140672/190551 (74%)]\tLoss: 1.984795\n",
      "Train Epoch: 9 [153472/190551 (81%)]\tLoss: 2.042973\n",
      "Train Epoch: 9 [166272/190551 (87%)]\tLoss: 2.010186\n",
      "Train Epoch: 9 [179072/190551 (94%)]\tLoss: 2.104984\n",
      "12.23s\n",
      "\n",
      "Test set: Average loss: 2.0634, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 10 [1280/190551 (1%)]\tLoss: 2.028518\n",
      "Train Epoch: 10 [14080/190551 (7%)]\tLoss: 2.073792\n",
      "Train Epoch: 10 [26880/190551 (14%)]\tLoss: 2.066887\n",
      "Train Epoch: 10 [39680/190551 (21%)]\tLoss: 2.120887\n",
      "Train Epoch: 10 [52480/190551 (28%)]\tLoss: 2.157532\n",
      "Train Epoch: 10 [65280/190551 (34%)]\tLoss: 2.063597\n",
      "Train Epoch: 10 [78080/190551 (41%)]\tLoss: 2.044893\n",
      "Train Epoch: 10 [90880/190551 (48%)]\tLoss: 2.072197\n",
      "Train Epoch: 10 [103680/190551 (54%)]\tLoss: 2.074473\n",
      "Train Epoch: 10 [116480/190551 (61%)]\tLoss: 1.987970\n",
      "Train Epoch: 10 [129280/190551 (68%)]\tLoss: 2.076769\n",
      "Train Epoch: 10 [142080/190551 (75%)]\tLoss: 2.143162\n",
      "Train Epoch: 10 [154880/190551 (81%)]\tLoss: 2.118467\n",
      "Train Epoch: 10 [167680/190551 (88%)]\tLoss: 2.027305\n",
      "Train Epoch: 10 [180480/190551 (95%)]\tLoss: 2.058609\n",
      "12.18s\n",
      "\n",
      "Test set: Average loss: 2.0634, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 11 [2688/190551 (1%)]\tLoss: 2.044051\n",
      "Train Epoch: 11 [15488/190551 (8%)]\tLoss: 2.077361\n",
      "Train Epoch: 11 [28288/190551 (15%)]\tLoss: 1.997226\n",
      "Train Epoch: 11 [41088/190551 (22%)]\tLoss: 1.935070\n",
      "Train Epoch: 11 [53888/190551 (28%)]\tLoss: 2.153037\n",
      "Train Epoch: 11 [66688/190551 (35%)]\tLoss: 2.028576\n",
      "Train Epoch: 11 [79488/190551 (42%)]\tLoss: 2.029455\n",
      "Train Epoch: 11 [92288/190551 (48%)]\tLoss: 2.043204\n",
      "Train Epoch: 11 [105088/190551 (55%)]\tLoss: 2.146669\n",
      "Train Epoch: 11 [117888/190551 (62%)]\tLoss: 1.980366\n",
      "Train Epoch: 11 [130688/190551 (69%)]\tLoss: 2.058445\n",
      "Train Epoch: 11 [143488/190551 (75%)]\tLoss: 1.908950\n",
      "Train Epoch: 11 [156288/190551 (82%)]\tLoss: 1.968822\n",
      "Train Epoch: 11 [169088/190551 (89%)]\tLoss: 1.988743\n",
      "Train Epoch: 11 [181888/190551 (95%)]\tLoss: 2.027855\n",
      "12.22s\n",
      "\n",
      "Test set: Average loss: 2.0636, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 12 [4096/190551 (2%)]\tLoss: 2.110171\n",
      "Train Epoch: 12 [16896/190551 (9%)]\tLoss: 2.097175\n",
      "Train Epoch: 12 [29696/190551 (16%)]\tLoss: 2.072091\n",
      "Train Epoch: 12 [42496/190551 (22%)]\tLoss: 1.934207\n",
      "Train Epoch: 12 [55296/190551 (29%)]\tLoss: 2.007275\n",
      "Train Epoch: 12 [68096/190551 (36%)]\tLoss: 2.042520\n",
      "Train Epoch: 12 [80896/190551 (42%)]\tLoss: 1.932986\n",
      "Train Epoch: 12 [93696/190551 (49%)]\tLoss: 2.183591\n",
      "Train Epoch: 12 [106496/190551 (56%)]\tLoss: 2.011849\n",
      "Train Epoch: 12 [119296/190551 (63%)]\tLoss: 1.981588\n",
      "Train Epoch: 12 [132096/190551 (69%)]\tLoss: 2.098399\n",
      "Train Epoch: 12 [144896/190551 (76%)]\tLoss: 2.152524\n",
      "Train Epoch: 12 [157696/190551 (83%)]\tLoss: 2.074178\n",
      "Train Epoch: 12 [170496/190551 (89%)]\tLoss: 2.075192\n",
      "Train Epoch: 12 [183296/190551 (96%)]\tLoss: 1.960226\n",
      "12.74s\n",
      "\n",
      "Test set: Average loss: 2.0631, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 13 [5504/190551 (3%)]\tLoss: 2.034597\n",
      "Train Epoch: 13 [18304/190551 (10%)]\tLoss: 2.122445\n",
      "Train Epoch: 13 [31104/190551 (16%)]\tLoss: 2.102154\n",
      "Train Epoch: 13 [43904/190551 (23%)]\tLoss: 1.950037\n",
      "Train Epoch: 13 [56704/190551 (30%)]\tLoss: 2.043392\n",
      "Train Epoch: 13 [69504/190551 (36%)]\tLoss: 2.056059\n",
      "Train Epoch: 13 [82304/190551 (43%)]\tLoss: 2.049923\n",
      "Train Epoch: 13 [95104/190551 (50%)]\tLoss: 2.026673\n",
      "Train Epoch: 13 [107904/190551 (57%)]\tLoss: 2.059600\n",
      "Train Epoch: 13 [120704/190551 (63%)]\tLoss: 2.010720\n",
      "Train Epoch: 13 [133504/190551 (70%)]\tLoss: 2.072330\n",
      "Train Epoch: 13 [146304/190551 (77%)]\tLoss: 2.166601\n",
      "Train Epoch: 13 [159104/190551 (83%)]\tLoss: 1.964356\n",
      "Train Epoch: 13 [171904/190551 (90%)]\tLoss: 1.995052\n",
      "Train Epoch: 13 [184704/190551 (97%)]\tLoss: 2.039253\n",
      "12.43s\n",
      "\n",
      "Test set: Average loss: 2.0621, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 14 [6912/190551 (4%)]\tLoss: 2.027046\n",
      "Train Epoch: 14 [19712/190551 (10%)]\tLoss: 1.989807\n",
      "Train Epoch: 14 [32512/190551 (17%)]\tLoss: 2.085560\n",
      "Train Epoch: 14 [45312/190551 (24%)]\tLoss: 2.043941\n",
      "Train Epoch: 14 [58112/190551 (30%)]\tLoss: 2.105584\n",
      "Train Epoch: 14 [70912/190551 (37%)]\tLoss: 2.119151\n",
      "Train Epoch: 14 [83712/190551 (44%)]\tLoss: 2.111882\n",
      "Train Epoch: 14 [96512/190551 (51%)]\tLoss: 2.136379\n",
      "Train Epoch: 14 [109312/190551 (57%)]\tLoss: 2.060881\n",
      "Train Epoch: 14 [122112/190551 (64%)]\tLoss: 2.061228\n",
      "Train Epoch: 14 [134912/190551 (71%)]\tLoss: 2.135021\n",
      "Train Epoch: 14 [147712/190551 (78%)]\tLoss: 2.043424\n",
      "Train Epoch: 14 [160512/190551 (84%)]\tLoss: 2.087913\n",
      "Train Epoch: 14 [173312/190551 (91%)]\tLoss: 1.982107\n",
      "Train Epoch: 14 [186112/190551 (98%)]\tLoss: 2.058657\n",
      "12.37s\n",
      "\n",
      "Test set: Average loss: 2.0617, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 15 [8320/190551 (4%)]\tLoss: 2.043394\n",
      "Train Epoch: 15 [21120/190551 (11%)]\tLoss: 2.056671\n",
      "Train Epoch: 15 [33920/190551 (18%)]\tLoss: 2.122706\n",
      "Train Epoch: 15 [46720/190551 (25%)]\tLoss: 2.042387\n",
      "Train Epoch: 15 [59520/190551 (31%)]\tLoss: 2.060799\n",
      "Train Epoch: 15 [72320/190551 (38%)]\tLoss: 2.117765\n",
      "Train Epoch: 15 [85120/190551 (45%)]\tLoss: 2.002167\n",
      "Train Epoch: 15 [97920/190551 (51%)]\tLoss: 2.021521\n",
      "Train Epoch: 15 [110720/190551 (58%)]\tLoss: 2.043357\n",
      "Train Epoch: 15 [123520/190551 (65%)]\tLoss: 1.970444\n",
      "Train Epoch: 15 [136320/190551 (72%)]\tLoss: 2.034281\n",
      "Train Epoch: 15 [149120/190551 (78%)]\tLoss: 2.143179\n",
      "Train Epoch: 15 [161920/190551 (85%)]\tLoss: 2.021616\n",
      "Train Epoch: 15 [174720/190551 (92%)]\tLoss: 2.041748\n",
      "Train Epoch: 15 [187520/190551 (98%)]\tLoss: 2.078784\n",
      "12.59s\n",
      "\n",
      "Test set: Average loss: 2.0607, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 16 [9728/190551 (5%)]\tLoss: 1.995122\n",
      "Train Epoch: 16 [22528/190551 (12%)]\tLoss: 2.136578\n",
      "Train Epoch: 16 [35328/190551 (19%)]\tLoss: 2.044400\n",
      "Train Epoch: 16 [48128/190551 (25%)]\tLoss: 2.126714\n",
      "Train Epoch: 16 [60928/190551 (32%)]\tLoss: 1.978400\n",
      "Train Epoch: 16 [73728/190551 (39%)]\tLoss: 2.017871\n",
      "Train Epoch: 16 [86528/190551 (45%)]\tLoss: 1.997238\n",
      "Train Epoch: 16 [99328/190551 (52%)]\tLoss: 2.025699\n",
      "Train Epoch: 16 [112128/190551 (59%)]\tLoss: 2.057930\n",
      "Train Epoch: 16 [124928/190551 (66%)]\tLoss: 2.044536\n",
      "Train Epoch: 16 [137728/190551 (72%)]\tLoss: 2.042599\n",
      "Train Epoch: 16 [150528/190551 (79%)]\tLoss: 2.062025\n",
      "Train Epoch: 16 [163328/190551 (86%)]\tLoss: 2.067873\n",
      "Train Epoch: 16 [176128/190551 (92%)]\tLoss: 1.996434\n",
      "Train Epoch: 16 [188928/190551 (99%)]\tLoss: 2.051805\n",
      "12.44s\n",
      "\n",
      "Test set: Average loss: 2.0602, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 17 [11136/190551 (6%)]\tLoss: 2.011861\n",
      "Train Epoch: 17 [23936/190551 (13%)]\tLoss: 1.995449\n",
      "Train Epoch: 17 [36736/190551 (19%)]\tLoss: 2.047956\n",
      "Train Epoch: 17 [49536/190551 (26%)]\tLoss: 1.992432\n",
      "Train Epoch: 17 [62336/190551 (33%)]\tLoss: 1.913588\n",
      "Train Epoch: 17 [75136/190551 (39%)]\tLoss: 2.105463\n",
      "Train Epoch: 17 [87936/190551 (46%)]\tLoss: 2.133593\n",
      "Train Epoch: 17 [100736/190551 (53%)]\tLoss: 2.123312\n",
      "Train Epoch: 17 [113536/190551 (60%)]\tLoss: 1.948799\n",
      "Train Epoch: 17 [126336/190551 (66%)]\tLoss: 2.183410\n",
      "Train Epoch: 17 [139136/190551 (73%)]\tLoss: 2.004188\n",
      "Train Epoch: 17 [151936/190551 (80%)]\tLoss: 2.097160\n",
      "Train Epoch: 17 [164736/190551 (86%)]\tLoss: 2.107831\n",
      "Train Epoch: 17 [177536/190551 (93%)]\tLoss: 2.089745\n",
      "Train Epoch: 17 [190336/190551 (100%)]\tLoss: 2.121603\n",
      "12.30s\n",
      "\n",
      "Test set: Average loss: 2.0599, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 18 [12544/190551 (7%)]\tLoss: 2.071941\n",
      "Train Epoch: 18 [25344/190551 (13%)]\tLoss: 2.049079\n",
      "Train Epoch: 18 [38144/190551 (20%)]\tLoss: 2.037265\n",
      "Train Epoch: 18 [50944/190551 (27%)]\tLoss: 1.920573\n",
      "Train Epoch: 18 [63744/190551 (33%)]\tLoss: 2.137211\n",
      "Train Epoch: 18 [76544/190551 (40%)]\tLoss: 2.109662\n",
      "Train Epoch: 18 [89344/190551 (47%)]\tLoss: 2.051641\n",
      "Train Epoch: 18 [102144/190551 (54%)]\tLoss: 1.985859\n",
      "Train Epoch: 18 [114944/190551 (60%)]\tLoss: 2.103970\n",
      "Train Epoch: 18 [127744/190551 (67%)]\tLoss: 1.995999\n",
      "Train Epoch: 18 [140544/190551 (74%)]\tLoss: 2.033957\n",
      "Train Epoch: 18 [153344/190551 (80%)]\tLoss: 2.026317\n",
      "Train Epoch: 18 [166144/190551 (87%)]\tLoss: 2.010457\n",
      "Train Epoch: 18 [178944/190551 (94%)]\tLoss: 2.065188\n",
      "12.37s\n",
      "\n",
      "Test set: Average loss: 2.0597, Accuracy: 2909/47638 (6%)\n",
      "\n",
      "Train Epoch: 19 [1152/190551 (1%)]\tLoss: 2.037066\n",
      "Train Epoch: 19 [13952/190551 (7%)]\tLoss: 2.023650\n",
      "Train Epoch: 19 [26752/190551 (14%)]\tLoss: 2.060737\n",
      "Train Epoch: 19 [39552/190551 (21%)]\tLoss: 2.004427\n",
      "Train Epoch: 19 [52352/190551 (27%)]\tLoss: 2.073872\n",
      "Train Epoch: 19 [65152/190551 (34%)]\tLoss: 2.074860\n",
      "Train Epoch: 19 [77952/190551 (41%)]\tLoss: 1.983297\n",
      "Train Epoch: 19 [90752/190551 (48%)]\tLoss: 2.030861\n",
      "Train Epoch: 19 [103552/190551 (54%)]\tLoss: 2.104909\n",
      "Train Epoch: 19 [116352/190551 (61%)]\tLoss: 2.026383\n",
      "Train Epoch: 19 [129152/190551 (68%)]\tLoss: 2.175193\n",
      "Train Epoch: 19 [141952/190551 (74%)]\tLoss: 2.012257\n",
      "Train Epoch: 19 [154752/190551 (81%)]\tLoss: 2.018084\n",
      "Train Epoch: 19 [167552/190551 (88%)]\tLoss: 1.973801\n",
      "Train Epoch: 19 [180352/190551 (95%)]\tLoss: 2.012084\n",
      "12.12s\n",
      "\n",
      "Test set: Average loss: 2.0594, Accuracy: 2909/47638 (6%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_loop(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatly, we were unable to create an efficient architecture for training, and much more tunning of the network is needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
