{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text classification using ConvNet\n",
    "Do the same, using a ConvNet.  \n",
    "The ConvNet should get as input a 2D matrix where each column is an embedding vector of a single word, and words are in order. Use zero padding so that all matrices have a similar length.  \n",
    "Some songs might be very long. Trim them so you keep a maximum of 128 words (after cleaning stop words and rare words).  \n",
    "Initialize the embedding layer using the word vectors that you've trained before, but allow them to change during training.  \n",
    "\n",
    "Extra: Try training the ConvNet with 2 slight modifications:\n",
    "1. freezing the the weights trained using Word2vec (preventing it from updating)\n",
    "1. random initialization of the embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are encouraged to try this question on your own.  \n",
    "\n",
    "You might prefer to get ideas from the paper \"Convolutional Neural Networks for Sentence Classification\" (Kim 2014, [link](https://arxiv.org/abs/1408.5882)).\n",
    "\n",
    "There are several implementations of the paper code in PyTorch online (see for example [this repo](https://github.com/prakashpandey9/Text-Classification-Pytorch) for a PyTorch implementation of CNN and other architectures for text classification). If you get stuck, they might provide you with a reference for your own code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data_utils\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import classification_report, plot_confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_FILE = 'lyrics.csv'\n",
    "DATA_DIR = 'data'\n",
    "MODELS_DIR = 'models'\n",
    "\n",
    "MAX_N_WORDS = 128\n",
    "MAX_FEATURES = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpochLogger(CallbackAny2Vec):\n",
    "    \"\"\"Callback to log information about training\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    \n",
    "w2v = Word2Vec.load(os.path.join(MODELS_DIR, 'w2v.model'))\n",
    "# w2v.init_sims(replace=True)\n",
    "\n",
    "df = pd.read_pickle(os.path.join(DATA_DIR, 'lyrics_df.pkl'))\n",
    "df.drop(df[df.genre == 'Not Available'].index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will prepare the data for training - we split the data into train and test, crop lyrics to the the required length and pad them if needed, and convert each lyrics to a one hot, 2d representation using CountVectorizor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_lyrics(row, n_words):\n",
    "    return row.clean_lyrics[:n_words]\n",
    "\n",
    "\n",
    "def lyrics_to_embedding(row, n_words, n_features, count_vect):\n",
    "    \n",
    "    embedding = count_vect.transform(row).toarray().argmax(axis=1)\n",
    "    \n",
    "    n_vects = embedding.shape[0]\n",
    "    if n_vects < n_words:\n",
    "        embedding = np.append(embedding, \n",
    "                              np.zeros(n_words - n_vects, dtype=int),\n",
    "                              axis=0)\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# df = df.iloc[idx].copy()\n",
    "df['genre_code'] = df.genre.astype('category').cat.codes\n",
    "df['cropped_lyrics'] = df.apply(clip_lyrics, args=(MAX_N_WORDS,), axis=1)\n",
    "genres_codes = df[['genre', 'genre_code']].drop_duplicates().set_index('genre_code').sort_index().squeeze()\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.cropped_lyrics, \n",
    "                                                    df.genre_code, \n",
    "                                                    test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 10.2 s, sys: 100 ms, total: 10.3 s\n",
      "Wall time: 10.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocab = w2v.wv.index2word[:MAX_FEATURES]\n",
    "count_vect = CountVectorizer(vocabulary=vocab).fit(X_train.str.join(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 17s, sys: 59.3 ms, total: 3min 17s\n",
      "Wall time: 3min 17s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "X_train = X_train.apply(lyrics_to_embedding, args=(MAX_N_WORDS, MAX_FEATURES, count_vect))\n",
    "X_test = X_test.apply(lyrics_to_embedding, args=(MAX_N_WORDS, MAX_FEATURES, count_vect))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train we convert the processed data into dataloaders to be used for our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_target = torch.tensor(y_train.values).long()\n",
    "train = torch.tensor(np.stack(X_train.values)).long()\n",
    "train_tensor = data_utils.TensorDataset(train, train_target) \n",
    "train_loader = data_utils.DataLoader(dataset=train_tensor, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_target = torch.tensor(y_test.values).long()\n",
    "test = torch.tensor(np.stack(X_test.values)).long()\n",
    "test_tensor = data_utils.TensorDataset(test, test_target) \n",
    "test_loader = data_utils.DataLoader(dataset=test_tensor, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define our model - the model will receive as input a 2d, one hot representation of the lyrics, as a sequence, and will make use of the word2vec embeddings, allowing them to change during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(epoch, log_interval=500):\n",
    "    model.train()  # set training mode\n",
    "#     lr = 5e-5\n",
    "    print(lr)\n",
    "    iteration = 1\n",
    "    for ep in range(1, epoch+1):\n",
    "#         if ep % 10==0:\n",
    "#             print(f'decreasing learning rate from {lr} to {lr/2}\\n')\n",
    "#             lr /= 2\n",
    "        optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr)\n",
    "        start = time.time()\n",
    "        \n",
    "        for batch_idx, (X, target) in enumerate(train_loader, start=1):\n",
    "            # bring data to the computing device, e.g. GPU\n",
    "            X, target = X.to(device), target.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            output = model(X)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (iteration % log_interval) == 0:\n",
    "                print(f'Epoch:{ep:>2}/{epoch}\\tBatch:{batch_idx:>5}/{len(train_loader)}',\n",
    "                      f'Samples:{batch_idx * len(X):>6}/{len(train_loader.dataset):<6} ({100 * batch_idx // len(train_loader):<3}%)', \n",
    "                      f'\\tLoss: {loss.item():.3f}\\tElapsed Time: {time.time()-start:.2f}s')\n",
    "        \n",
    "            iteration += 1\n",
    "            \n",
    "        test(plot_cm=(ep == epoch)) # evaluate at the end of epoch\n",
    "        print(f'{\"=\"*50} \\nElapsed time: {time.time()-start:.2f}s\\n')\n",
    "        \n",
    "def test(plot_cm=False):\n",
    "    model.eval()  # set evaluation mode\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predictions = []\n",
    "    confusion_matrix = torch.zeros(model.n_labels, model.n_labels)\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item() # sum up batch loss\n",
    "            pred = output.max(1)[1] # get the index of the max log-probability\n",
    "            \n",
    "            correct += (pred == target).sum().item()\n",
    "            total += target.size(0)\n",
    "            for t, p in zip(target, pred):\n",
    "                confusion_matrix[t, p] += 1\n",
    "    \n",
    "    test_loss /= len(test_loader)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))\n",
    "    \n",
    "    if plot_cm:\n",
    "        \n",
    "        class_correct = confusion_matrix.diag()\n",
    "        class_total = confusion_matrix.sum(1)\n",
    "        class_accuracies = 100 * class_correct / class_total\n",
    "        \n",
    "        for code, label in genres_codes.items():\n",
    "            confusion_matrix[code] = confusion_matrix[code] / confusion_matrix[code].sum()\n",
    "            print(f'Accuracy for {label}: {class_accuracies[code]:.3f}%')\n",
    "        \n",
    "        print(f'\\nMean per class accuracy: {class_accuracies.mean():.3f}%')\n",
    "\n",
    "        # Plot confusion matrix\n",
    "        f = plt.figure()\n",
    "        ax = f.add_subplot(111)\n",
    "        cax = ax.imshow(confusion_matrix.numpy(), interpolation='nearest', cmap=plt.cm.Blues)\n",
    "        f.colorbar(cax)\n",
    "        plt.xticks(range(genres_codes.size), genres_codes.to_list(), rotation=90)\n",
    "        plt.yticks(range(genres_codes.size), genres_codes.to_list())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class ConvNet(nn.Module):\n",
    "    def __init__(self, n_labels, initial_weights=None, freeze_weights=False,\n",
    "                 in_channels=1, out_channels=100, \n",
    "                 kernels=[3,4,5], padding=0, stride=1, \n",
    "                 keep_probab = 0.5,\n",
    "                ):\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        self.n_labels = n_labels\n",
    "        self.allow_grad = True\n",
    "        \n",
    "        if initial_weights is None:\n",
    "            raise NotImplementedError('Random Initialization Not Implemented Yet')\n",
    "        else:\n",
    "            self.init_weights = torch.tensor(initial_weights, dtype=torch.float)\n",
    "            self.allow_grad = not freeze_weights\n",
    "            vocab_size, embedding_length = initial_weights.shape\n",
    "\n",
    "        \n",
    "        self.kernels = np.asarray(kernels)\n",
    "                \n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n",
    "        self.word_embeddings.weight = nn.Parameter(self.init_weights, requires_grad=True)\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, \n",
    "                               kernel_size=(kernels[0], embedding_length), \n",
    "                               stride=embedding_length, \n",
    "                               )\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels, out_channels, \n",
    "                               kernel_size=(kernels[1], embedding_length), \n",
    "                               stride=embedding_length, \n",
    "                               )\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels, out_channels, \n",
    "                               kernel_size=(kernels[2], embedding_length), \n",
    "                               stride=embedding_length, \n",
    "                               )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=keep_probab)\n",
    "        self.label = nn.Linear(len(kernels)*out_channels, n_labels)\n",
    "        \n",
    "    def _conv_block(self, x, conv_layer):\n",
    "        conv_out = conv_layer(x)  # conv_out.size() = (batch_size, out_channels, dim, 1)\n",
    "        activation = F.relu(conv_out.squeeze(3))  # activation.size() = (batch_size, out_channels, dim1)\n",
    "        max_out = F.max_pool1d(activation, activation.size()[2]).squeeze(2)  # maxpool_out.size() = (batch_size, out_channels)\n",
    "        return max_out\n",
    "\n",
    "    def forward(self, x):\n",
    "    \n",
    "        x = self.word_embeddings(x)\n",
    "        out = x.unsqueeze(1)\n",
    "        max_out1 = self._conv_block(out, self.conv1)\n",
    "        max_out2 = self._conv_block(out, self.conv2)\n",
    "        max_out3 = self._conv_block(out, self.conv3)\n",
    "\n",
    "        all_out = torch.cat((max_out1, max_out2, max_out3), 1)\n",
    "        all_out = all_out.view(all_out.size(0), -1)\n",
    "        fc_in = self.dropout(all_out)\n",
    "        \n",
    "        scores = self.label(fc_in)\n",
    "        \n",
    "#         scores = F.softmax(scores, dim=1)\n",
    "        return scores\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Use GPU if available, otherwise stick with cpu\n",
    "use_cuda = torch.cuda.is_available()\n",
    "torch.manual_seed(42)\n",
    "if use_cuda:\n",
    "    torch.cuda.manual_seed(42)\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = np.unique(y_train).size\n",
    "bow_vocab = count_vect.vocabulary  \n",
    "\n",
    "model = ConvNet(initial_weights=w2v.wv[bow_vocab], n_labels=output_size).to(device)\n",
    "optimizer = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-5)\n",
    "\n",
    "weights = torch.Tensor(y_train.value_counts().max() / y_train.value_counts().sort_index()).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7.4922, 14.1662, 51.1544,  4.3104, 34.2876, 13.5466,  4.6881, 20.5281,\n",
       "         2.6576, 30.9370,  1.0000])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.Tensor(y_train.value_counts().max() / y_train.value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n",
      "Epoch: 1/5\tBatch:  500/5955 Samples: 16000/190551 (8  %) \tLoss: 2.237\tElapsed Time: 1.58s\n",
      "Epoch: 1/5\tBatch: 1000/5955 Samples: 32000/190551 (16 %) \tLoss: 2.063\tElapsed Time: 3.16s\n",
      "Epoch: 1/5\tBatch: 1500/5955 Samples: 48000/190551 (25 %) \tLoss: 2.223\tElapsed Time: 4.68s\n",
      "Epoch: 1/5\tBatch: 2000/5955 Samples: 64000/190551 (33 %) \tLoss: 2.092\tElapsed Time: 6.20s\n",
      "Epoch: 1/5\tBatch: 2500/5955 Samples: 80000/190551 (41 %) \tLoss: 2.167\tElapsed Time: 7.71s\n",
      "Epoch: 1/5\tBatch: 3000/5955 Samples: 96000/190551 (50 %) \tLoss: 2.255\tElapsed Time: 9.21s\n",
      "Epoch: 1/5\tBatch: 3500/5955 Samples:112000/190551 (58 %) \tLoss: 2.440\tElapsed Time: 10.74s\n",
      "Epoch: 1/5\tBatch: 4000/5955 Samples:128000/190551 (67 %) \tLoss: 1.965\tElapsed Time: 12.27s\n",
      "Epoch: 1/5\tBatch: 4500/5955 Samples:144000/190551 (75 %) \tLoss: 2.191\tElapsed Time: 13.80s\n",
      "Epoch: 1/5\tBatch: 5000/5955 Samples:160000/190551 (83 %) \tLoss: 2.177\tElapsed Time: 15.33s\n",
      "Epoch: 1/5\tBatch: 5500/5955 Samples:176000/190551 (92 %) \tLoss: 2.374\tElapsed Time: 16.87s\n",
      "\n",
      "Test set: Average loss: 2.1325, Accuracy: 10021/47638 (21%)\n",
      "\n",
      "================================================== \n",
      "Elapsed time: 23.04s\n",
      "\n",
      "Epoch: 2/5\tBatch:   45/5955 Samples:  1440/190551 (0  %) \tLoss: 1.928\tElapsed Time: 0.17s\n",
      "Epoch: 2/5\tBatch:  545/5955 Samples: 17440/190551 (9  %) \tLoss: 2.197\tElapsed Time: 1.68s\n",
      "Epoch: 2/5\tBatch: 1045/5955 Samples: 33440/190551 (17 %) \tLoss: 2.258\tElapsed Time: 3.22s\n",
      "Epoch: 2/5\tBatch: 1545/5955 Samples: 49440/190551 (25 %) \tLoss: 2.152\tElapsed Time: 4.74s\n",
      "Epoch: 2/5\tBatch: 2045/5955 Samples: 65440/190551 (34 %) \tLoss: 2.239\tElapsed Time: 6.38s\n",
      "Epoch: 2/5\tBatch: 2545/5955 Samples: 81440/190551 (42 %) \tLoss: 1.990\tElapsed Time: 8.04s\n",
      "Epoch: 2/5\tBatch: 3045/5955 Samples: 97440/190551 (51 %) \tLoss: 1.665\tElapsed Time: 9.57s\n",
      "Epoch: 2/5\tBatch: 3545/5955 Samples:113440/190551 (59 %) \tLoss: 2.428\tElapsed Time: 11.09s\n",
      "Epoch: 2/5\tBatch: 4045/5955 Samples:129440/190551 (67 %) \tLoss: 1.918\tElapsed Time: 12.81s\n",
      "Epoch: 2/5\tBatch: 4545/5955 Samples:145440/190551 (76 %) \tLoss: 2.091\tElapsed Time: 14.35s\n",
      "Epoch: 2/5\tBatch: 5045/5955 Samples:161440/190551 (84 %) \tLoss: 1.972\tElapsed Time: 15.93s\n",
      "Epoch: 2/5\tBatch: 5545/5955 Samples:177440/190551 (93 %) \tLoss: 2.065\tElapsed Time: 17.52s\n",
      "\n",
      "Test set: Average loss: 2.0994, Accuracy: 10245/47638 (22%)\n",
      "\n",
      "================================================== \n",
      "Elapsed time: 24.50s\n",
      "\n",
      "Epoch: 3/5\tBatch:   90/5955 Samples:  2880/190551 (1  %) \tLoss: 2.312\tElapsed Time: 0.33s\n",
      "Epoch: 3/5\tBatch:  590/5955 Samples: 18880/190551 (9  %) \tLoss: 1.808\tElapsed Time: 2.01s\n",
      "Epoch: 3/5\tBatch: 1090/5955 Samples: 34880/190551 (18 %) \tLoss: 1.935\tElapsed Time: 3.63s\n",
      "Epoch: 3/5\tBatch: 1590/5955 Samples: 50880/190551 (26 %) \tLoss: 2.103\tElapsed Time: 5.35s\n",
      "Epoch: 3/5\tBatch: 2090/5955 Samples: 66880/190551 (35 %) \tLoss: 1.992\tElapsed Time: 6.93s\n",
      "Epoch: 3/5\tBatch: 2590/5955 Samples: 82880/190551 (43 %) \tLoss: 1.730\tElapsed Time: 8.62s\n",
      "Epoch: 3/5\tBatch: 3090/5955 Samples: 98880/190551 (51 %) \tLoss: 2.133\tElapsed Time: 10.20s\n",
      "Epoch: 3/5\tBatch: 3590/5955 Samples:114880/190551 (60 %) \tLoss: 1.979\tElapsed Time: 12.03s\n",
      "Epoch: 3/5\tBatch: 4090/5955 Samples:130880/190551 (68 %) \tLoss: 1.907\tElapsed Time: 13.75s\n",
      "Epoch: 3/5\tBatch: 4590/5955 Samples:146880/190551 (77 %) \tLoss: 2.051\tElapsed Time: 15.48s\n",
      "Epoch: 3/5\tBatch: 5090/5955 Samples:162880/190551 (85 %) \tLoss: 2.028\tElapsed Time: 17.11s\n",
      "Epoch: 3/5\tBatch: 5590/5955 Samples:178880/190551 (93 %) \tLoss: 1.846\tElapsed Time: 18.68s\n",
      "\n",
      "Test set: Average loss: 2.1011, Accuracy: 11781/47638 (25%)\n",
      "\n",
      "================================================== \n",
      "Elapsed time: 24.68s\n",
      "\n",
      "Epoch: 4/5\tBatch:  135/5955 Samples:  4320/190551 (2  %) \tLoss: 1.642\tElapsed Time: 0.48s\n",
      "Epoch: 4/5\tBatch:  635/5955 Samples: 20320/190551 (10 %) \tLoss: 2.228\tElapsed Time: 2.14s\n",
      "Epoch: 4/5\tBatch: 1135/5955 Samples: 36320/190551 (19 %) \tLoss: 2.140\tElapsed Time: 3.93s\n",
      "Epoch: 4/5\tBatch: 1635/5955 Samples: 52320/190551 (27 %) \tLoss: 2.152\tElapsed Time: 5.68s\n",
      "Epoch: 4/5\tBatch: 2135/5955 Samples: 68320/190551 (35 %) \tLoss: 1.621\tElapsed Time: 7.20s\n",
      "Epoch: 4/5\tBatch: 2635/5955 Samples: 84320/190551 (44 %) \tLoss: 2.128\tElapsed Time: 8.73s\n",
      "Epoch: 4/5\tBatch: 3135/5955 Samples:100320/190551 (52 %) \tLoss: 1.785\tElapsed Time: 10.39s\n",
      "Epoch: 4/5\tBatch: 3635/5955 Samples:116320/190551 (61 %) \tLoss: 1.903\tElapsed Time: 12.06s\n",
      "Epoch: 4/5\tBatch: 4135/5955 Samples:132320/190551 (69 %) \tLoss: 2.254\tElapsed Time: 13.65s\n",
      "Epoch: 4/5\tBatch: 4635/5955 Samples:148320/190551 (77 %) \tLoss: 1.947\tElapsed Time: 15.29s\n",
      "Epoch: 4/5\tBatch: 5135/5955 Samples:164320/190551 (86 %) \tLoss: 2.223\tElapsed Time: 16.94s\n",
      "Epoch: 4/5\tBatch: 5635/5955 Samples:180320/190551 (94 %) \tLoss: 1.476\tElapsed Time: 18.74s\n",
      "\n",
      "Test set: Average loss: 2.1172, Accuracy: 10690/47638 (22%)\n",
      "\n",
      "================================================== \n",
      "Elapsed time: 25.37s\n",
      "\n",
      "Epoch: 5/5\tBatch:  180/5955 Samples:  5760/190551 (3  %) \tLoss: 1.838\tElapsed Time: 0.62s\n",
      "Epoch: 5/5\tBatch:  680/5955 Samples: 21760/190551 (11 %) \tLoss: 1.519\tElapsed Time: 2.22s\n",
      "Epoch: 5/5\tBatch: 1180/5955 Samples: 37760/190551 (19 %) \tLoss: 1.732\tElapsed Time: 3.99s\n",
      "Epoch: 5/5\tBatch: 1680/5955 Samples: 53760/190551 (28 %) \tLoss: 2.092\tElapsed Time: 5.57s\n",
      "Epoch: 5/5\tBatch: 2180/5955 Samples: 69760/190551 (36 %) \tLoss: 1.760\tElapsed Time: 7.29s\n",
      "Epoch: 5/5\tBatch: 2680/5955 Samples: 85760/190551 (45 %) \tLoss: 1.682\tElapsed Time: 8.90s\n",
      "Epoch: 5/5\tBatch: 3180/5955 Samples:101760/190551 (53 %) \tLoss: 1.348\tElapsed Time: 10.63s\n",
      "Epoch: 5/5\tBatch: 3680/5955 Samples:117760/190551 (61 %) \tLoss: 2.054\tElapsed Time: 12.13s\n",
      "Epoch: 5/5\tBatch: 4180/5955 Samples:133760/190551 (70 %) \tLoss: 1.714\tElapsed Time: 13.77s\n",
      "Epoch: 5/5\tBatch: 4680/5955 Samples:149760/190551 (78 %) \tLoss: 1.573\tElapsed Time: 15.29s\n",
      "Epoch: 5/5\tBatch: 5180/5955 Samples:165760/190551 (86 %) \tLoss: 1.832\tElapsed Time: 16.89s\n",
      "Epoch: 5/5\tBatch: 5680/5955 Samples:181760/190551 (95 %) \tLoss: 1.779\tElapsed Time: 18.61s\n",
      "\n",
      "Test set: Average loss: 2.1594, Accuracy: 9757/47638 (20%)\n",
      "\n",
      "Accuracy for Country: 38.157%\n",
      "Accuracy for Electronic: 26.689%\n",
      "Accuracy for Folk: 22.459%\n",
      "Accuracy for Hip-Hop: 51.750%\n",
      "Accuracy for Indie: 17.105%\n",
      "Accuracy for Jazz: 30.754%\n",
      "Accuracy for Metal: 47.607%\n",
      "Accuracy for Other: 16.567%\n",
      "Accuracy for Pop: 11.003%\n",
      "Accuracy for R&B: 27.955%\n",
      "Accuracy for Rock: 7.503%\n",
      "\n",
      "Mean per class accuracy: 27.050%\n",
      "================================================== \n",
      "Elapsed time: 24.25s\n",
      "\n",
      "Training completed!!!\n",
      "Total training time: 121.84069085121155\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "N_EPOCHS = 5\n",
    "lr = 1e-4\n",
    "t0 = time.time()\n",
    "train_loop(N_EPOCHS)\n",
    "print(f'Training completed!!!\\nTotal training time: {time.time() - t0}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunatly, we were unable to create an efficient architecture for training, and much more tunning of the network is needed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
